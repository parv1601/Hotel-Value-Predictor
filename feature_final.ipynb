{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "87a40ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split # Used if you were splitting here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "81a78684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (1200, 81)\n",
      "Testing Data Shape: (260, 80)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "print(\"Training Data Shape:\", df_train.shape)\n",
    "print(\"Testing Data Shape:\", df_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "dbff2908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save 'Id' columns for submission\n",
    "test_ID = df_test['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "e269e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original 'Id' column from the dataframes\n",
    "df_train.drop(\"Id\", axis=1, inplace=True)\n",
    "df_test.drop(\"Id\", axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44803bff",
   "metadata": {},
   "source": [
    "Outlier Variable Values in Training Data Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "52f91953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows removed due to extreme outliers: 6\n"
     ]
    }
   ],
   "source": [
    "# Remove samples based on Target Value and key predictors\n",
    "X_train_raw = df_train.drop(columns=['HotelValue'])\n",
    "y_train_raw = df_train['HotelValue']\n",
    "initial_row_count = len(df_train)\n",
    "\n",
    "# 1. Target-based cleaning: Remove extreme values (bottom 0.1% and top 0.1%)\n",
    "y_lower_bound = y_train_raw.quantile(0.001)\n",
    "y_upper_bound = y_train_raw.quantile(0.999)\n",
    "outlier_mask = (y_train_raw >= y_lower_bound) & (y_train_raw <= y_upper_bound)\n",
    "\n",
    "# 2. Predictor-based cleaning\n",
    "if 'UsableArea' in df_train.columns:\n",
    "    outlier_mask &= (df_train['UsableArea'] < 4000)\n",
    "\n",
    "if 'OverallQuality' in df_train.columns and 'UsableArea' in df_train.columns:\n",
    "    outlier_mask &= ~((df_train['OverallQuality'] < 3) & (df_train['UsableArea'] > 3000))\n",
    "\n",
    "# Apply the mask to *df_train* as well\n",
    "df_train = df_train[outlier_mask].copy()\n",
    "\n",
    "# Update X_train and y_train after cleaning\n",
    "X_train = df_train.drop(columns=['HotelValue']).copy()\n",
    "y_train = df_train['HotelValue'].copy()\n",
    "\n",
    "print(f\"Rows removed due to extreme outliers: {initial_row_count - len(df_train)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "247d0a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Predictor-based cleaning (Common for this type of dataset)\n",
    "# # Remove properties with extremely large UsableArea (e.g., > 4000 sq ft)\n",
    "# if 'UsableArea' in X_train_raw.columns:\n",
    "#     outlier_mask &= (X_train_raw['UsableArea'] < 4000)\n",
    "\n",
    "# # Remove properties with poor OverallQuality and high UsableArea (often errors)\n",
    "# if 'OverallQuality' in X_train_raw.columns and 'UsableArea' in X_train_raw.columns:\n",
    "#     outlier_mask &= ~((X_train_raw['OverallQuality'] < 3) & (X_train_raw['UsableArea'] > 3000))\n",
    "\n",
    "# # Apply the mask to both features and target\n",
    "# X_train = X_train_raw[outlier_mask].copy()\n",
    "# y_train = y_train_raw[outlier_mask].copy()\n",
    "\n",
    "# df_train = df_train[outlier_mask].copy()\n",
    "\n",
    "\n",
    "# print(f\"Rows removed due to extreme outliers: {initial_row_count - len(X_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "a27defba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PropertyClass         0\n",
      "ZoningCategory        0\n",
      "RoadAccessLength    223\n",
      "LandArea              0\n",
      "RoadType              0\n",
      "                   ... \n",
      "MonthSold             0\n",
      "YearSold              0\n",
      "DealType              0\n",
      "DealCondition         0\n",
      "HotelValue            0\n",
      "Length: 80, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "missing_counts = df_train.isna().sum().astype(np.int64)\n",
    "print(missing_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c25a9ad",
   "metadata": {},
   "source": [
    "Clearing the columns with many NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "290707c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraFacility      1148\n",
      "ServiceLaneType    1119\n",
      "BoundaryFence       960\n",
      "LoungeQuality       558\n",
      "FacadeType          699\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "missing_counts = (\n",
    "    df_train[['ExtraFacility', 'ServiceLaneType', 'BoundaryFence','LoungeQuality','FacadeType']]\n",
    "    .isna()\n",
    "    .sum()\n",
    "    .astype(np.int64)\n",
    ")\n",
    "\n",
    "print(missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "e82110b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraFacility      252\n",
      "ServiceLaneType    244\n",
      "BoundaryFence      216\n",
      "LoungeQuality      130\n",
      "FacadeType         170\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_counts = (\n",
    "    df_test[['ExtraFacility', 'ServiceLaneType', 'BoundaryFence','LoungeQuality','FacadeType']]\n",
    "    .isna()\n",
    "    .sum()\n",
    "    .astype(np.int64)\n",
    ")\n",
    "\n",
    "print(missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "c26847cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1194, 80)\n",
      "(260, 79)\n",
      "(1194, 75)\n",
      "(260, 74)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "df_train.drop(columns=[\n",
    "    'ExtraFacility', 'ServiceLaneType', \n",
    "    'BoundaryFence', 'FacadeType','LoungeQuality'\n",
    "],errors='ignore', inplace=True)\n",
    "\n",
    "df_test.drop(columns=[\n",
    "    'ExtraFacility', 'ServiceLaneType', \n",
    "    'BoundaryFence', 'FacadeType','LoungeQuality'\n",
    "],errors='ignore', inplace=True)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258ac958",
   "metadata": {},
   "source": [
    "For some columns very less amount of data is missing so its better to remove those rows as it will confuse our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "3bb1c1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectricalSystem    1\n",
      "dtype: int64\n",
      "ElectricalSystem    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df_train[['ElectricalSystem']].isna().sum().astype(np.int64))\n",
    "print(df_test[['ElectricalSystem']].isna().sum().astype(np.int64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "6da6f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(subset=['ElectricalSystem'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6398435",
   "metadata": {},
   "source": [
    "Checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "be60926c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df_train.duplicated().sum())\n",
    "print(df_test.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2462d",
   "metadata": {},
   "source": [
    "Possible incosistencies :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bd9b9f",
   "metadata": {},
   "source": [
    "\n",
    "RenovationYear < ConstructionYear\n",
    "\n",
    "YearSold < ConstructionYear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "a1434707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 1193\n",
      "Cleaned rows: 1193\n",
      "Rows removed: 0\n",
      "Original rows: 260\n",
      "Cleaned rows: 260\n",
      "Rows removed: 0\n"
     ]
    }
   ],
   "source": [
    "df_cleaned1 = df_train[\n",
    "    (df_train['RenovationYear'] >= df_train['ConstructionYear']) &\n",
    "    (df_train['YearSold'] >= df_train['ConstructionYear'])\n",
    "].copy()\n",
    "\n",
    "print(\"Original rows:\", len(df_train))\n",
    "print(\"Cleaned rows:\", len(df_cleaned1))\n",
    "print(\"Rows removed:\", len(df_train) - len(df_cleaned1))\n",
    "\n",
    "df_train = df_cleaned1\n",
    "\n",
    "\n",
    "df_cleaned2 = df_test[\n",
    "    (df_test['RenovationYear'] >= df_test['ConstructionYear']) &\n",
    "    (df_test['YearSold'] >= df_test['ConstructionYear'])\n",
    "].copy()\n",
    "\n",
    "print(\"Original rows:\", len(df_test))\n",
    "print(\"Cleaned rows:\", len(df_cleaned2))\n",
    "print(\"Rows removed:\", len(df_test) - len(df_cleaned2))\n",
    "\n",
    "df_test = df_cleaned2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3d397",
   "metadata": {},
   "source": [
    "Creating temporal features and removing original features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388f96d8",
   "metadata": {},
   "source": [
    "For Basement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "e4ca83e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging basement features...\n"
     ]
    }
   ],
   "source": [
    "# Merge Basement Features into Weighted Quality Score\n",
    "print(\"\\nMerging basement features...\")\n",
    "basement_quality_map = {\n",
    "    'GLQ': 5, 'ALQ': 4, 'BLQ': 3, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0\n",
    "}\n",
    "\n",
    "# Fill NaN values\n",
    "df_train['BasementFacilitySF1'] = df_train['BasementFacilitySF1'].fillna(0)\n",
    "df_train['BasementFacilitySF2'] = df_train['BasementFacilitySF2'].fillna(0)\n",
    "\n",
    "# Map types to scores\n",
    "df_train['Type1_Score'] = df_train['BasementFacilityType1'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "df_train['Type2_Score'] = df_train['BasementFacilityType2'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "\n",
    "# Calculate weighted quality score\n",
    "df_train['TotalBasementScore'] = (df_train['Type1_Score'] * df_train['BasementFacilitySF1']) + (df_train['Type2_Score'] * df_train['BasementFacilitySF2'])\n",
    "df_train['BasementFinishedSF'] = df_train['BasementFacilitySF1'] + df_train['BasementFacilitySF2']\n",
    "\n",
    "# Drop original basement facility columns\n",
    "df_train.drop(columns=['BasementFacilityType1', 'BasementFacilityType2', \n",
    "                 'BasementFacilitySF1', 'BasementFacilitySF2',\n",
    "                 'Type1_Score', 'Type2_Score'], errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "7135a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values\n",
    "df_test['BasementFacilitySF1'] = df_test['BasementFacilitySF1'].fillna(0)\n",
    "df_test['BasementFacilitySF2'] = df_test['BasementFacilitySF2'].fillna(0)\n",
    "\n",
    "# Map types to scores\n",
    "df_test['Type1_Score'] = df_test['BasementFacilityType1'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "df_test['Type2_Score'] = df_test['BasementFacilityType2'].fillna('None').map(basement_quality_map).fillna(0)\n",
    "\n",
    "# Calculate weighted quality score\n",
    "df_test['TotalBasementScore'] = (df_test['Type1_Score'] * df_test['BasementFacilitySF1']) + (df_test['Type2_Score'] * df_test['BasementFacilitySF2'])\n",
    "df_test['BasementFinishedSF'] = df_test['BasementFacilitySF1'] + df_test['BasementFacilitySF2']\n",
    "\n",
    "# Drop original basement facility columns\n",
    "df_test.drop(columns=['BasementFacilityType1', 'BasementFacilityType2', \n",
    "                 'BasementFacilitySF1', 'BasementFacilitySF2',\n",
    "                 'Type1_Score', 'Type2_Score'], errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250e9fd6",
   "metadata": {},
   "source": [
    "For Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "5e22caf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering Pool features...\n"
     ]
    }
   ],
   "source": [
    "print(\"Engineering Pool features...\")\n",
    "# Define a quality map for PoolQuality. \n",
    "# 'None' (or NaN) = 0, 'Fa' (Fair) = 1, 'Ex' (Excellent) = 4.\n",
    "# Added 'TA' (Typical) and 'Gd' (Good) as they are common.\n",
    "pool_quality_map = {\n",
    "    'None': 0,\n",
    "    'Fa': 1,\n",
    "    'Ex': 2,\n",
    "}\n",
    "\n",
    "# Fill NaN values first. 'PoolArea' NaNs mean 0 area.\n",
    "df_train['SwimmingPoolArea'] = df_train['SwimmingPoolArea'].fillna(0)\n",
    "df_train['PoolQuality'] = df_train['PoolQuality'].fillna('None')\n",
    "\n",
    "# Map quality strings to numeric scores\n",
    "df_train['PoolQuality_Score'] = df_train['PoolQuality'].map(pool_quality_map).fillna(0)\n",
    "\n",
    "# Create the new feature by multiplying quality by area\n",
    "df_train['TotalPoolScore'] = df_train['PoolQuality_Score'] * df_train['SwimmingPoolArea']\n",
    "\n",
    "# Now drop the original columns since they are combined\n",
    "df_train.drop(columns=['PoolQuality', 'SwimmingPoolArea','PoolQuality_Score'],\n",
    "         errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "85d4ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values first. 'PoolArea' NaNs mean 0 area.\n",
    "df_test['SwimmingPoolArea'] = df_test['SwimmingPoolArea'].fillna(0)\n",
    "df_test['PoolQuality'] = df_test['PoolQuality'].fillna('None')\n",
    "\n",
    "# Map quality strings to numeric scores\n",
    "df_test['PoolQuality_Score'] = df_test['PoolQuality'].map(pool_quality_map).fillna(0)\n",
    "\n",
    "# Create the new feature by multiplying quality by area\n",
    "df_test['TotalPoolScore'] = df_test['PoolQuality_Score'] * df_test['SwimmingPoolArea']\n",
    "\n",
    "# Now drop the original columns since they are combined\n",
    "df_test.drop(columns=['PoolQuality', 'SwimmingPoolArea','PoolQuality_Score'],\n",
    "         errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d905e",
   "metadata": {},
   "source": [
    "For Open Porch/Veranda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "ec2e24b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging porch features...\n"
     ]
    }
   ],
   "source": [
    "# Merge Porch/Veranda Features\n",
    "print(\"Merging porch features...\")\n",
    "\n",
    "df_train['TotalPorchArea'] = (\n",
    "    df_train['OpenVerandaArea'].fillna(0)+\n",
    "    df_train['EnclosedVerandaArea'].fillna(0) + \n",
    "    df_train['SeasonalPorchArea'].fillna(0) + \n",
    "    df_train['ScreenPorchArea'].fillna(0)\n",
    ")\n",
    "df_train.drop(columns=['OpenVerandaArea','EnclosedVerandaArea', 'SeasonalPorchArea', 'ScreenPorchArea'], \n",
    "         errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "d4493cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['TotalPorchArea'] = (\n",
    "    df_test['OpenVerandaArea'].fillna(0)+\n",
    "    df_test['EnclosedVerandaArea'].fillna(0) + \n",
    "    df_test['SeasonalPorchArea'].fillna(0) + \n",
    "    df_test['ScreenPorchArea'].fillna(0)\n",
    ")\n",
    "df_test.drop(columns=['OpenVerandaArea','EnclosedVerandaArea', 'SeasonalPorchArea', 'ScreenPorchArea'], \n",
    "         errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "0d781728",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train['TotalOutdoorArea'] = (df_train['TerraceArea'] + df_train['TotalPorchArea']).fillna(0)\n",
    "df_train['TotalSF'] = (df_train['GroundFloorArea'] + df_train['UpperFloorArea'] + df_train['ParkingArea'] + df_train['TotalOutdoorArea']).fillna(0)\n",
    "\n",
    "df_train['TotalBaths'] = (df_train['FullBaths'] + 0.5 * df_train['HalfBaths'] +\n",
    "                    df_train['BasementFullBaths'] + 0.5 * df_train['BasementHalfBaths']).fillna(0)\n",
    "\n",
    "df_train['OverallScore'] = (df_train['OverallQuality'] + df_train['OverallCondition']) / 2.0 # Assumes these columns were NOT dropped\n",
    "\n",
    "# --- 2. Temporal Features ---\n",
    "df_train['Age'] = df_train['YearSold'] - df_train['ConstructionYear']\n",
    "df_train['YearsSinceRemodel'] = df_train['YearSold'] - df_train['RenovationYear']\n",
    "df_train['YearsSinceRemodel'] = np.where(df_train['YearsSinceRemodel'] < 0, 0, df_train['YearsSinceRemodel'])\n",
    "df_train.loc[df_train['RenovationYear'] == df_train['ConstructionYear'], 'YearsSinceRemodel'] = df_train['Age']\n",
    "\n",
    "# --- 3. Interaction Feature (Example) ---\n",
    "df_train['Qual_x_GroundSF'] = df_train['OverallQuality'] * df_train['GroundFloorArea'] # Assumes these columns were NOT dropped\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "812bcbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['TotalOutdoorArea'] = (df_test['TerraceArea'] + df_test['TotalPorchArea']).fillna(0)\n",
    "df_test['TotalSF'] = (df_test['GroundFloorArea'] + df_test['UpperFloorArea'] + df_test['ParkingArea'] + df_test['TotalOutdoorArea']).fillna(0)\n",
    "\n",
    "df_test['TotalBaths'] = (df_test['FullBaths'] + 0.5 * df_test['HalfBaths'] +\n",
    "                    df_test['BasementFullBaths'] + 0.5 * df_test['BasementHalfBaths']).fillna(0)\n",
    "\n",
    "df_test['OverallScore'] = (df_test['OverallQuality'] + df_test['OverallCondition']) / 2.0 # Assumes these columns were NOT dropped\n",
    "\n",
    "# --- 2. Temporal Features ---\n",
    "df_test['Age'] = df_test['YearSold'] - df_test['ConstructionYear']\n",
    "df_test['YearsSinceRemodel'] = df_test['YearSold'] - df_test['RenovationYear']\n",
    "df_test['YearsSinceRemodel'] = np.where(df_test['YearsSinceRemodel'] < 0, 0, df_test['YearsSinceRemodel'])\n",
    "df_test.loc[df_test['RenovationYear'] == df_test['ConstructionYear'], 'YearsSinceRemodel'] = df_test['Age']\n",
    "\n",
    "# --- 3. Interaction Feature (Example) ---\n",
    "df_test['Qual_x_GroundSF'] = df_test['OverallQuality'] * df_test['GroundFloorArea'] # Assumes these columns were NOT dropped\n",
    "\n",
    "# --- 4. Feature Reduction/Drop ---\n",
    "drop_cols = ['GroundFloorArea', 'UpperFloorArea', \n",
    "                'ConstructionYear', 'RenovationYear', \n",
    "                'FullBaths', 'HalfBaths','ParkingArea',\n",
    "                'BasementFullBaths', 'BasementHalfBaths',\n",
    "                'TerraceArea', 'OverallQuality', 'OverallCondition','MonthSold','YearSold']\n",
    "df_train.drop(columns=drop_cols, inplace=True)\n",
    "df_test.drop(columns=drop_cols, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e8969f",
   "metadata": {},
   "source": [
    "Giving New Ordinal Mapping to Parameters for Better comparing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b72bc",
   "metadata": {},
   "source": [
    "Parking Features New Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "9d356ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW ORDINAL PARKING MAPPING ---\n",
    "# Define maps for ordinal parking features\n",
    "quality_map_5pt = {\n",
    "    'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None': 0\n",
    "}\n",
    "parking_finish_map = {\n",
    "    'Fin': 3, 'RFn': 2, 'Unf': 1, 'None': 0\n",
    "}\n",
    "\n",
    "# Overwrite categorical columns with their new numerical scores\n",
    "\n",
    "# Impute and map Quality\n",
    "df_train['ParkingQuality'] = df_train['ParkingQuality'].fillna('None').map(quality_map_5pt).fillna(0)\n",
    "\n",
    "# Impute and map Condition\n",
    "df_train['ParkingCondition'] = df_train['ParkingCondition'].fillna('None').map(quality_map_5pt).fillna(0)\n",
    "\n",
    "# Impute and map Finish\n",
    "df_train['ParkingFinish'] = df_train['ParkingFinish'].fillna('None').map(parking_finish_map).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "41e6f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute and map Quality\n",
    "df_test['ParkingQuality'] = df_test['ParkingQuality'].fillna('None').map(quality_map_5pt).fillna(0)\n",
    "\n",
    "# Impute and map Condition\n",
    "df_test['ParkingCondition'] = df_test['ParkingCondition'].fillna('None').map(quality_map_5pt).fillna(0)\n",
    "\n",
    "# Impute and map Finish\n",
    "df_test['ParkingFinish'] = df_test['ParkingFinish'].fillna('None').map(parking_finish_map).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d531134",
   "metadata": {},
   "source": [
    "New Conditional Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "5b1b53e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW PROPERTY FUNCTIONALITY MAPPING ---\n",
    "# This feature represents deductions from 'Typical'\n",
    "functionality_map = {\n",
    "    'Typ': 7,  # Typical\n",
    "    'Min1': 6, # Minor Deductions 1\n",
    "    'Min2': 5, # Minor Deductions 2\n",
    "    'Mod': 4,  # Moderate Deductions\n",
    "    'Maj1': 3, # Major Deductions 1\n",
    "    'Maj2': 2, # Major Deductions 2\n",
    "    'Sev': 1,  # Severely Damaged\n",
    "    'None': 0  # Assuming 'None' is worse than 'Sev' or not applicable\n",
    "}\n",
    "##--- NEW EXTERIOR QUALITY/CONDITION MAPPING ---\n",
    "df_train['ExteriorQuality'] = df_train['ExteriorQuality'].fillna('None').map(quality_map_5pt).fillna(0)\n",
    "df_train['ExteriorCondition'] = df_train['ExteriorCondition'].fillna('None').map(quality_map_5pt).fillna(0)\n",
    "\n",
    "# BasementCondition (uses 5-point map)\n",
    "df_train['BasementCondition'] = df_train['BasementCondition'].fillna('None').map(quality_map_5pt).fillna(0)\n",
    "\n",
    "# BasementExposure (custom map)\n",
    "exposure_map = {\n",
    "    'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'None': 0\n",
    "}\n",
    "df_train['BasementExposure'] = df_train['BasementExposure'].fillna('None').map(exposure_map).fillna(0)\n",
    "\n",
    "# --- NEW KITCHEN/HEATING QUALITY MAPPING ---\n",
    "df_train['KitchenQuality'] = df_train['KitchenQuality'].fillna('None').map(quality_map_5pt).fillna(0)\n",
    "df_train['HeatingQuality'] = df_train['HeatingQuality'].fillna('None').map(quality_map_5pt).fillna(0)\n",
    "# --- END NEW KITCHEN/HEATING SECTION ---\n",
    "# Impute and map PropertyFunctionality. \n",
    "# Use fillna('Typ') if 'None' should be treated as 'Typical'\n",
    "df_train['PropertyFunctionality'] = df_train['PropertyFunctionality'].fillna('None').map(functionality_map).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "b20b0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##--- NEW EXTERIOR QUALITY/CONDITION MAPPING ---\n",
    "df_test['ExteriorQuality'] = df_test['ExteriorQuality'].fillna('None').map(quality_map_5pt).fillna(0)\n",
    "df_test['ExteriorCondition'] = df_test['ExteriorCondition'].fillna('None').map(quality_map_5pt).fillna(0)\n",
    "\n",
    "# BasementCondition (uses 5-point map)\n",
    "df_test['BasementCondition'] = df_test['BasementCondition'].fillna('None').map(quality_map_5pt).fillna(0)\n",
    "\n",
    "df_test['BasementExposure'] = df_test['BasementExposure'].fillna('None').map(exposure_map).fillna(0)\n",
    "\n",
    "# --- NEW KITCHEN/HEATING QUALITY MAPPING ---\n",
    "df_test['KitchenQuality'] = df_test['KitchenQuality'].fillna('None').map(quality_map_5pt).fillna(0)\n",
    "df_test['HeatingQuality'] = df_test['HeatingQuality'].fillna('None').map(quality_map_5pt).fillna(0)\n",
    "# --- END NEW KITCHEN/HEATING SECTION ---\n",
    "# Impute and map PropertyFunctionality. \n",
    "# Use fillna('Typ') if 'None' should be treated as 'Typical'\n",
    "df_test['PropertyFunctionality'] = df_test['PropertyFunctionality'].fillna('None').map(functionality_map).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b8442",
   "metadata": {},
   "source": [
    "Normalize the skewed HotelValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "b1723d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['HotelValue_Log'] = np.log1p(y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f029df8",
   "metadata": {},
   "source": [
    "Pre Processed CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "8c0b515f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1193, 63)\n",
      "(260, 61)\n",
      "PropertyClass                0\n",
      "ZoningCategory               0\n",
      "RoadAccessLength           223\n",
      "LandArea                     0\n",
      "RoadType                     0\n",
      "PlotShape                    0\n",
      "LandElevation                0\n",
      "UtilityAccess                0\n",
      "PlotConfiguration            0\n",
      "LandSlope                    0\n",
      "District                     0\n",
      "NearbyTransport1             0\n",
      "NearbyTransport2             0\n",
      "PropertyType                 0\n",
      "HotelStyle                   0\n",
      "RoofDesign                   0\n",
      "RoofMaterial                 0\n",
      "ExteriorPrimary              0\n",
      "ExteriorSecondary            0\n",
      "FacadeArea                   7\n",
      "ExteriorQuality              0\n",
      "ExteriorCondition            0\n",
      "FoundationType               0\n",
      "BasementHeight              29\n",
      "BasementCondition            0\n",
      "BasementExposure             0\n",
      "BasementUnfinishedSF         0\n",
      "BasementTotalSF              0\n",
      "HeatingType                  0\n",
      "HeatingQuality               0\n",
      "CentralAC                    0\n",
      "ElectricalSystem             0\n",
      "LowQualityArea               0\n",
      "UsableArea                   0\n",
      "GuestRooms                   0\n",
      "Kitchens                     0\n",
      "KitchenQuality               0\n",
      "TotalRooms                   0\n",
      "PropertyFunctionality        0\n",
      "Lounges                      0\n",
      "ParkingType                 63\n",
      "ParkingConstructionYear     63\n",
      "ParkingFinish                0\n",
      "ParkingCapacity              0\n",
      "ParkingQuality               0\n",
      "ParkingCondition             0\n",
      "DrivewayType                 0\n",
      "ExtraFacilityValue           0\n",
      "DealType                     0\n",
      "DealCondition                0\n",
      "HotelValue                   0\n",
      "TotalBasementScore           0\n",
      "BasementFinishedSF           0\n",
      "TotalPoolScore               0\n",
      "TotalPorchArea               0\n",
      "TotalOutdoorArea             0\n",
      "TotalSF                      0\n",
      "TotalBaths                   0\n",
      "OverallScore                 0\n",
      "Age                          0\n",
      "YearsSinceRemodel            0\n",
      "Qual_x_GroundSF              0\n",
      "HotelValue_Log               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train.to_csv('preprocesssed_train.csv',index=False)\n",
    "df_test.to_csv('preprocesssed_test.csv',index=False)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "pd.set_option('display.max_rows',None)\n",
    "\n",
    "missing_values = df_train.isna().sum().astype(np.int64)\n",
    "\n",
    "print(missing_values)\n",
    "\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "3e245021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your currently saved processed data\n",
    "# X_train = pd.read_csv('preprocesssed_train.csv')\n",
    "# X_test = pd.read_csv('preprocesssed_test.csv')\n",
    "# y_train_log = X_train['HotelValue_Log']\n",
    "# y_train = X_train['HotelValue']\n",
    "# X_train.drop(columns=['HotelValue_Log', 'HotelValue'], inplace=True)\n",
    "\n",
    "# print(\"--- Columns in X_train ---\")\n",
    "# # Use the .tolist() method for a clean, simple list output\n",
    "# print(X_train.columns.tolist())\n",
    "\n",
    "# print(\"\\n--- Columns in X_test ---\")\n",
    "# print(X_test.columns.tolist())\n",
    "\n",
    "# numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
    "# categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# numerical_features_test = X_test.select_dtypes(include=np.number).columns.tolist()\n",
    "# categorical_features_test = X_test.select_dtypes(include=['object', 'category']).columns.tolist\n",
    "\n",
    "# print(\"\\n--- 3. Feature Engineering Complete ---\")\n",
    "# print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "# print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "# print(f\"Final training features shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "d98a0738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# # Numerical Transformer: Impute, Scale, and add Polynomial Features\n",
    "# numerical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='median')),\n",
    "#     ('scaler', StandardScaler()), \n",
    "#     # The degree=2 poly features were commented out in your previous code to speed things up. \n",
    "#     # I'll keep them commented unless performance is a concern.\n",
    "#     # ('poly', PolynomialFeatures(degree=2, include_bias=False)) \n",
    "# ])\n",
    "\n",
    "\n",
    "# # Categorical Transformer: Impute and One-Hot Encode\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='constant', fill_value='None')), \n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "# ])\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numerical_transformer, numerical_features),\n",
    "#         ('cat', categorical_transformer, categorical_features)\n",
    "#     ],\n",
    "#     remainder='drop'\n",
    "# )\n",
    "\n",
    "# preprocessor.fit(X_train)\n",
    "\n",
    "# X_train_transformed = preprocessor.transform(X_train)\n",
    "# X_train_processed_df = pd.DataFrame(X_train_transformed)\n",
    "\n",
    "# X_test_transformed = preprocessor.transform(X_test)\n",
    "# X_test_processed_df = pd.DataFrame(X_test_transformed)\n",
    "\n",
    "# # Reattach the target to the training data\n",
    "# X_train_processed_df['HotelValue_Log'] = y_train_log.values \n",
    "# X_train_processed_df['HotelValue'] = y_train.values \n",
    "\n",
    "# X_train_processed_df.fillna(0, inplace=True) \n",
    "# X_test_processed_df.fillna(0, inplace=True)\n",
    "\n",
    "# # --- 4. Final Save ---\n",
    "# X_train_processed_df.to_csv('final_processed_train.csv', index=False)\n",
    "# X_test_processed_df.to_csv('final_processed_test.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "dd922c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11863/2971996459.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_final.fillna(0, inplace=True)\n",
      "/tmp/ipykernel_11863/2971996459.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_final.fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X_train = pd.read_csv('preprocesssed_train.csv')\n",
    "X_test = pd.read_csv('preprocesssed_test.csv')\n",
    "y_train_log = X_train['HotelValue_Log']\n",
    "y_train = X_train['HotelValue']\n",
    "X_train.drop(columns=['HotelValue_Log', 'HotelValue'], inplace=True)\n",
    "\n",
    "# 1. Combine for Unified One-Hot Encoding\n",
    "combined_df = pd.concat([X_train, X_test], axis=0, ignore_index=True)\n",
    "\n",
    "# 2. Convert all remaining object columns (Nominal Categorical) to dummy variables\n",
    "X_combined_encoded = pd.get_dummies(combined_df, drop_first=True)\n",
    "\n",
    "# 3. Separate back into train/test sets\n",
    "# Note: The shapes of these DataFrames should now be identical (except for the row count)\n",
    "X_train_final = X_combined_encoded.iloc[:len(X_train)]\n",
    "X_test_final = X_combined_encoded.iloc[len(X_train):]\n",
    "\n",
    "# 4. Final Sanity Check and NaN Guard (Essential Fix)\n",
    "# This step ensures no NaNs survive encoding/scaling and caused your previous ValueError.\n",
    "X_train_final.fillna(0, inplace=True)\n",
    "X_test_final.fillna(0, inplace=True)\n",
    "\n",
    "# 5. Scaling (Fit on Train, Transform on Test)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit only on TRAIN and transform both\n",
    "X_train_scaled = scaler.fit_transform(X_train_final)\n",
    "X_test_scaled = scaler.transform(X_test_final)\n",
    "\n",
    "# Convert back to DataFrames\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train_final.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test_final.columns)\n",
    "\n",
    "# 6. Final Save (Attach target to train set)\n",
    "X_train_scaled_df['HotelValue_Log'] = y_train_log.values \n",
    " \n",
    "X_train_scaled_df.to_csv('final_processed_train.csv', index=False)\n",
    "X_test_scaled_df.to_csv('final_processed_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "95c1fdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1193, 183)\n",
      "(260, 182)\n",
      "PropertyClass                0\n",
      "RoadAccessLength             0\n",
      "LandArea                     0\n",
      "FacadeArea                   0\n",
      "ExteriorQuality              0\n",
      "ExteriorCondition            0\n",
      "BasementCondition            0\n",
      "BasementExposure             0\n",
      "BasementUnfinishedSF         0\n",
      "BasementTotalSF              0\n",
      "HeatingQuality               0\n",
      "LowQualityArea               0\n",
      "UsableArea                   0\n",
      "GuestRooms                   0\n",
      "Kitchens                     0\n",
      "KitchenQuality               0\n",
      "TotalRooms                   0\n",
      "PropertyFunctionality        0\n",
      "Lounges                      0\n",
      "ParkingConstructionYear      0\n",
      "ParkingFinish                0\n",
      "ParkingCapacity              0\n",
      "ParkingQuality               0\n",
      "ParkingCondition             0\n",
      "ExtraFacilityValue           0\n",
      "TotalBasementScore           0\n",
      "BasementFinishedSF           0\n",
      "TotalPoolScore               0\n",
      "TotalPorchArea               0\n",
      "TotalOutdoorArea             0\n",
      "TotalSF                      0\n",
      "TotalBaths                   0\n",
      "OverallScore                 0\n",
      "Age                          0\n",
      "YearsSinceRemodel            0\n",
      "Qual_x_GroundSF              0\n",
      "ZoningCategory_FV            0\n",
      "ZoningCategory_RH            0\n",
      "ZoningCategory_RL            0\n",
      "ZoningCategory_RM            0\n",
      "RoadType_Pave                0\n",
      "PlotShape_IR2                0\n",
      "PlotShape_IR3                0\n",
      "PlotShape_Reg                0\n",
      "LandElevation_HLS            0\n",
      "LandElevation_Low            0\n",
      "LandElevation_Lvl            0\n",
      "UtilityAccess_NoSeWa         0\n",
      "PlotConfiguration_CulDSac    0\n",
      "PlotConfiguration_FR2        0\n",
      "PlotConfiguration_FR3        0\n",
      "PlotConfiguration_Inside     0\n",
      "LandSlope_Mod                0\n",
      "LandSlope_Sev                0\n",
      "District_Blueste             0\n",
      "District_BrDale              0\n",
      "District_BrkSide             0\n",
      "District_ClearCr             0\n",
      "District_CollgCr             0\n",
      "District_Crawfor             0\n",
      "District_Edwards             0\n",
      "District_Gilbert             0\n",
      "District_IDOTRR              0\n",
      "District_MeadowV             0\n",
      "District_Mitchel             0\n",
      "District_NAmes               0\n",
      "District_NPkVill             0\n",
      "District_NWAmes              0\n",
      "District_NoRidge             0\n",
      "District_NridgHt             0\n",
      "District_OldTown             0\n",
      "District_SWISU               0\n",
      "District_Sawyer              0\n",
      "District_SawyerW             0\n",
      "District_Somerst             0\n",
      "District_StoneBr             0\n",
      "District_Timber              0\n",
      "District_Veenker             0\n",
      "NearbyTransport1_Feedr       0\n",
      "NearbyTransport1_Norm        0\n",
      "NearbyTransport1_PosA        0\n",
      "NearbyTransport1_PosN        0\n",
      "NearbyTransport1_RRAe        0\n",
      "NearbyTransport1_RRAn        0\n",
      "NearbyTransport1_RRNe        0\n",
      "NearbyTransport1_RRNn        0\n",
      "NearbyTransport2_Feedr       0\n",
      "NearbyTransport2_Norm        0\n",
      "NearbyTransport2_PosA        0\n",
      "NearbyTransport2_PosN        0\n",
      "NearbyTransport2_RRAe        0\n",
      "NearbyTransport2_RRAn        0\n",
      "NearbyTransport2_RRNn        0\n",
      "PropertyType_2fmCon          0\n",
      "PropertyType_Duplex          0\n",
      "PropertyType_Twnhs           0\n",
      "PropertyType_TwnhsE          0\n",
      "HotelStyle_1.5Unf            0\n",
      "HotelStyle_1Story            0\n",
      "HotelStyle_2.5Fin            0\n",
      "HotelStyle_2.5Unf            0\n",
      "HotelStyle_2Story            0\n",
      "HotelStyle_SFoyer            0\n",
      "HotelStyle_SLvl              0\n",
      "RoofDesign_Gable             0\n",
      "RoofDesign_Gambrel           0\n",
      "RoofDesign_Hip               0\n",
      "RoofDesign_Mansard           0\n",
      "RoofDesign_Shed              0\n",
      "RoofMaterial_Membran         0\n",
      "RoofMaterial_Metal           0\n",
      "RoofMaterial_Roll            0\n",
      "RoofMaterial_Tar&Grv         0\n",
      "RoofMaterial_WdShake         0\n",
      "RoofMaterial_WdShngl         0\n",
      "ExteriorPrimary_AsphShn      0\n",
      "ExteriorPrimary_BrkComm      0\n",
      "ExteriorPrimary_BrkFace      0\n",
      "ExteriorPrimary_CBlock       0\n",
      "ExteriorPrimary_CemntBd      0\n",
      "ExteriorPrimary_HdBoard      0\n",
      "ExteriorPrimary_ImStucc      0\n",
      "ExteriorPrimary_MetalSd      0\n",
      "ExteriorPrimary_Plywood      0\n",
      "ExteriorPrimary_Stone        0\n",
      "ExteriorPrimary_Stucco       0\n",
      "ExteriorPrimary_VinylSd      0\n",
      "ExteriorPrimary_Wd Sdng      0\n",
      "ExteriorPrimary_WdShing      0\n",
      "ExteriorSecondary_AsphShn    0\n",
      "ExteriorSecondary_Brk Cmn    0\n",
      "ExteriorSecondary_BrkFace    0\n",
      "ExteriorSecondary_CBlock     0\n",
      "ExteriorSecondary_CmentBd    0\n",
      "ExteriorSecondary_HdBoard    0\n",
      "ExteriorSecondary_ImStucc    0\n",
      "ExteriorSecondary_MetalSd    0\n",
      "ExteriorSecondary_Other      0\n",
      "ExteriorSecondary_Plywood    0\n",
      "ExteriorSecondary_Stone      0\n",
      "ExteriorSecondary_Stucco     0\n",
      "ExteriorSecondary_VinylSd    0\n",
      "ExteriorSecondary_Wd Sdng    0\n",
      "ExteriorSecondary_Wd Shng    0\n",
      "FoundationType_CBlock        0\n",
      "FoundationType_PConc         0\n",
      "FoundationType_Slab          0\n",
      "FoundationType_Stone         0\n",
      "FoundationType_Wood          0\n",
      "BasementHeight_Fa            0\n",
      "BasementHeight_Gd            0\n",
      "BasementHeight_TA            0\n",
      "HeatingType_GasA             0\n",
      "HeatingType_GasW             0\n",
      "HeatingType_Grav             0\n",
      "HeatingType_OthW             0\n",
      "HeatingType_Wall             0\n",
      "CentralAC_Y                  0\n",
      "ElectricalSystem_FuseF       0\n",
      "ElectricalSystem_FuseP       0\n",
      "ElectricalSystem_Mix         0\n",
      "ElectricalSystem_SBrkr       0\n",
      "ParkingType_Attchd           0\n",
      "ParkingType_Basment          0\n",
      "ParkingType_BuiltIn          0\n",
      "ParkingType_CarPort          0\n",
      "ParkingType_Detchd           0\n",
      "DrivewayType_P               0\n",
      "DrivewayType_Y               0\n",
      "DealType_CWD                 0\n",
      "DealType_Con                 0\n",
      "DealType_ConLD               0\n",
      "DealType_ConLI               0\n",
      "DealType_ConLw               0\n",
      "DealType_New                 0\n",
      "DealType_Oth                 0\n",
      "DealType_WD                  0\n",
      "DealCondition_AdjLand        0\n",
      "DealCondition_Alloca         0\n",
      "DealCondition_Family         0\n",
      "DealCondition_Normal         0\n",
      "DealCondition_Partial        0\n",
      "HotelValue_Log               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('final_processed_train.csv')\n",
    "df_test = pd.read_csv('final_processed_test.csv')\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "pd.set_option('display.max_rows',None)\n",
    "\n",
    "missing_values = df_train.isna().sum().astype(np.int64)\n",
    "\n",
    "print(missing_values)\n",
    "\n",
    "pd.reset_option('display.max_rows')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
